{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Prediction using LSTM\n",
    "I've seen similar projects to this try to generate text that reads like shakespeare\n",
    "\n",
    "How is this different?\n",
    "* I wanted text that was easier to judge, so I scraped data from NSF which are project abstracts\n",
    "* In order to better learn and demonstrate the foundational parts of Tensorflow, I didn't use tflearn\n",
    "\n",
    "We start out with doing some simple imports and defining how the training/test data will be read and displayed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn import *\n",
    "\n",
    "char_to_index = {}\n",
    "index_to_char = {}\n",
    "\n",
    "def array_info(description, x):\n",
    "    \"\"\" Displays description and shape of the provided numpy array\n",
    "    \"\"\"\n",
    "    return \"Array \" + description + \" \" + str(x.shape);\n",
    "    \n",
    "def array_to_str(char_array):\n",
    "    \"\"\" Converts an array of character indices to a Python string\n",
    "    \"\"\"\n",
    "    return \"\".join([index_to_char[c] for c in char_array])\n",
    "\n",
    "def display_strs(chars_2d):\n",
    "    \"\"\" Displays an array of strings\n",
    "    \"\"\"\n",
    "    count = chars_2d.shape[0]\n",
    "    split = np.split(chars_2d, count)\n",
    "    for chars in split:\n",
    "        print(array_to_str(chars[0]))\n",
    "        \n",
    "def read_textfile_to_arrays(path, x_len=25, y_len=2, skip_factor=3, to_lower_case=False, train_percent=99):\n",
    "    global char_to_index;\n",
    "    global index_to_char;\n",
    "    global char_count;\n",
    "    \n",
    "    total_len = x_len+y_len\n",
    "    \n",
    "    # Read the whole text file\n",
    "    string = open(path).read()\n",
    "    if to_lower_case:\n",
    "        string = string.lower()\n",
    "\n",
    "    chars = sorted(list(set(string)))\n",
    "    char_count = len(chars)\n",
    "    char_to_index = {chars[i]: i for i in range(char_count)}  # maps from ascii codes to an index\n",
    "    index_to_char = {v: k for k, v in char_to_index.items()}\n",
    "\n",
    "    length = len(string)\n",
    "    count = (length-total_len)//skip_factor\n",
    "\n",
    "    x_all = np.zeros((count, x_len), dtype=np.int32)\n",
    "    y_all = np.zeros((count, y_len), dtype=np.int32)\n",
    "\n",
    "    for s in range(0, count):\n",
    "        i = s * skip_factor\n",
    "        for t, char in enumerate(string[i: i + x_len]):\n",
    "            x_all[s][t] = char_to_index[char]\n",
    "        for t, char in enumerate(string[i+x_len: i + total_len]):\n",
    "            y_all[s][t] = char_to_index[char]\n",
    "\n",
    "    test_index = count - count * train_percent//100\n",
    "\n",
    "    x_train = x_all[test_index:]\n",
    "    y_train = y_all[test_index:]\n",
    "    x_test = x_all[:test_index]\n",
    "    y_test = y_all[:test_index]\n",
    "\n",
    "    print(\"Text total length: \" + str(length))\n",
    "    print(\"Distinct chars: \" + str(len(chars)))\n",
    "    print(\"Total sequences: \" + str(count))\n",
    "\n",
    "    print(array_info(\"x_train\", x_train))\n",
    "    print(array_info(\"y_train\", y_train))\n",
    "    print(array_info(\"x_test\", x_test))\n",
    "    print(array_info(\"y_test\", y_test))\n",
    "\n",
    "    i=2;\n",
    "    print(\"Example at row: %d X[%d]: '%s'  Y[%d]: '%s'\" % (i, i, array_to_str(x_train[i]),\n",
    "                                       i, array_to_str(y_train[i])))\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data\n",
    "Defines the length of input and output that the text will be divided into\n",
    "\n",
    "Defines the location of where the data is stored\n",
    "\n",
    "Loads the data\n",
    "\n",
    "Prints information on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text total length: 18062680\n",
      "Distinct chars: 98\n",
      "Total sequences: 180626\n",
      "Array x_train (178819, 40)\n",
      "Array y_train (178819, 1)\n",
      "Array x_test (1807, 40)\n",
      "Array y_test (1807, 1)\n",
      "Example at row: 2 X[2]: 'research at the Field Museum. The PI giv'  Y[2]: 'e'\n",
      "index_to_char\n",
      "0->'\\t'  1->'\\n'  2->' '  3->'!'  4->'\"'  5->'#'  6->'$'  7->'%'  8->'&'  9->''' \n",
      "10->'('  11->')'  12->'*'  13->'+'  14->','  15->'-'  16->'.'  17->'/'  18->'0'  19->'1' \n",
      "20->'2'  21->'3'  22->'4'  23->'5'  24->'6'  25->'7'  26->'8'  27->'9'  28->':'  29->';' \n",
      "30->'<'  31->'='  32->'>'  33->'?'  34->'@'  35->'A'  36->'B'  37->'C'  38->'D'  39->'E' \n",
      "40->'F'  41->'G'  42->'H'  43->'I'  44->'J'  45->'K'  46->'L'  47->'M'  48->'N'  49->'O' \n",
      "50->'P'  51->'Q'  52->'R'  53->'S'  54->'T'  55->'U'  56->'V'  57->'W'  58->'X'  59->'Y' \n",
      "60->'Z'  61->'['  62->'\\'  63->']'  64->'^'  65->'_'  66->'`'  67->'a'  68->'b'  69->'c' \n",
      "70->'d'  71->'e'  72->'f'  73->'g'  74->'h'  75->'i'  76->'j'  77->'k'  78->'l'  79->'m' \n",
      "80->'n'  81->'o'  82->'p'  83->'q'  84->'r'  85->'s'  86->'t'  87->'u'  88->'v'  89->'w' \n",
      "90->'x'  91->'y'  92->'z'  93->'{'  94->'|'  95->'}'  96->'~'  97->'ï¿½'  "
     ]
    }
   ],
   "source": [
    "seq_len = 40\n",
    "y_len = 1\n",
    "\n",
    "data_dir = \"/home/eric/Documents/data/NSF/\"\n",
    "data_file_name = \"nsf_file_clean_10000.txt\" \n",
    "data_path = data_dir + data_file_name\n",
    "\n",
    "# Actually load the data\n",
    "x_train, y_train, x_test, y_test = read_textfile_to_arrays(data_path, x_len=seq_len, y_len=y_len, skip_factor=100)\n",
    "\n",
    "# We are only dealing with the first character of the expected output\n",
    "y_train = y_train[:, 0];\n",
    "y_test = y_test[:, 0];\n",
    "\n",
    "print(\"index_to_char\")\n",
    "for i in range(char_count):\n",
    "    c = str(index_to_char[i])\n",
    "    if c[0]=='\\t': c = '\\\\t'\n",
    "    if c[0]=='\\n': c = '\\\\n'\n",
    "    print(\"%d->'%s' %c\" % (i, c, '\\n' if (i%10==9) else ' '), end='')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model\n",
    "Uses a fully connected layer as input to the LSTM layers\n",
    "\n",
    "Four LSTM layers are in the middle which provides the recurrance so that a subsequent character can hold a state based on previous characters\n",
    "\n",
    "Final layer reduces size to match the number of possible characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_neurons = 512\n",
    "num_layers = 4\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope(\"TextModel\"):\n",
    "    # define input placeholders\n",
    "    x_input = tf.placeholder(\"int32\", [None, seq_len])\n",
    "    target_output = tf.placeholder(\"int32\", [None])\n",
    "    keep_prob = tf.placeholder(\"float32\")\n",
    "\n",
    "    # convert to 1-hot\n",
    "    x_input_1hot = tf.one_hot(x_input, char_count, on_value=1.0, off_value=0.0, dtype=tf.float32)\n",
    "    ideal_output_1hot = tf.one_hot(target_output, char_count, on_value=1.0, off_value=0.0, dtype=tf.float32)\n",
    "\n",
    "    # transform x_input_1hot which is (N, seq_len, char_count) => (N, seq_len, num_neurons)\n",
    "    result = tf.reshape(x_input_1hot, [-1, char_count])\n",
    "    w_0 = tf.Variable(tf.random_normal([char_count, num_neurons], stddev=0.01))\n",
    "    result = tf.matmul(result, w_0)\n",
    "    result = tf.reshape(result, [-1, seq_len, num_neurons])\n",
    "\n",
    "    cells = []\n",
    "    for _ in range(num_layers):\n",
    "        mycell = LSTMCell(num_neurons, state_is_tuple=True)  # or BasicLSTMCell\n",
    "        mycell = DropoutWrapper(mycell, output_keep_prob=keep_prob)\n",
    "        cells.append(mycell)\n",
    "\n",
    "    multi_cell = MultiRNNCell(cells, state_is_tuple=True)  # simple way of stacking multiple identical layers\n",
    "\n",
    "    output, _ = tf.nn.dynamic_rnn(multi_cell, result, dtype=tf.float32)\n",
    "\n",
    "    # transpose so that the sequence is the last dimension (N, num_neurons, seq_len)\n",
    "    output = tf.transpose(output, [0, 2, 1])\n",
    "    # truncate so we only get the last output from the sequence\n",
    "    output = output[:, :, seq_len -1]\n",
    "\n",
    "    # define transforming matrix with bias to convert (N, num_neurons) => (N, char_count)\n",
    "    bias_1 = tf.Variable(tf.random_normal([char_count], stddev=0.01))\n",
    "    w_1 = tf.Variable(tf.random_normal([num_neurons, char_count], stddev=0.01))\n",
    "\n",
    "    # note that we don't take the softmax at the end because our cost fn does that for us\n",
    "    output = tf.add(tf.matmul(output, w_1), bias_1)\n",
    "\n",
    "    # compute costs\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=ideal_output_1hot))\n",
    "    \n",
    "    # use an optimizer that has goal of reducing cost\n",
    "    train_op = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    # Selects a single output with some randomness\n",
    "    # large values (e.g. 10) are effectively selecting the argmax and low values are effectively random (e.g. .1)\n",
    "    randomness = 2  \n",
    "    predict_op = tf.multinomial(output * randomness, 1)[:,0]  # Selects the last character\n",
    "\n",
    "    init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing Predictions\n",
    "Defines a helper function that will show a few lines of predicted text\n",
    "Each line's text is a prediction based on the initial text which is provided\n",
    "Provided text comes from the test data set, so the data hasn't been seen before in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_predictions(sess):\n",
    "    prediction_row_offset = 1000\n",
    "    prediction_row_count = 10\n",
    "    display_row_count = 10  # not all of the predicted lines need to be displayed\n",
    "    prediction_char_count = 60\n",
    "\n",
    "    chars = np.append(x_test[prediction_row_offset:prediction_row_offset + prediction_row_count, :], \n",
    "                      np.zeros(shape=(prediction_row_count, prediction_char_count)), axis=1)\n",
    "\n",
    "    for t in range(prediction_char_count):\n",
    "        subchar = chars[:, t:t+seq_len]\n",
    "        predictions = sess.run(predict_op, feed_dict={x_input: subchar, keep_prob: 1.0})\n",
    "\n",
    "        # Take the generated output and add it on to the end of the array\n",
    "        chars[:, t + seq_len] = predictions\n",
    "    # Display a header line to indicate which characters are input (X) and which are predictions (Y)\n",
    "    print(\"X\"*seq_len + \"Y\"*prediction_char_count)\n",
    "    display_strs(chars[0:display_row_count, \n",
    "                       0:seq_len + prediction_char_count])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Training\n",
    "Loads state from any previous executions (so we don't start from scratch every time)\n",
    "\n",
    "After each epoch it performs\n",
    "* Measures accuracy from testing set\n",
    "* Shows examples of predicted strings (e.g. adds 60 predicted characters)\n",
    "* Saves its state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/eric/Documents/models/text_prediction_NSF\n",
      "Model loaded from: '/home/eric/Documents/models/text_prediction_NSF'\n",
      "\n",
      "Initial Predictions Before Training\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY\n",
      "inhibitor of the sugar transport protein information new development of the proposed research is a t\n",
      "sugars via pores acclimate to cold or high second molecular processing between the initial organic g\n",
      "a transport protein component. The focus of the proposed project will have experimental and properti\n",
      "haracterizing a range of additional structures and the process need and a research in the both stude\n",
      " capacity, including the number and cross-scale clears to an accordition and the control of this wor\n",
      "f pore connections, and membrane features and the proposed study of the proposed students will be co\n",
      "se structural features may be augmented by the system in collective systems are the research in the \n",
      "conditions, and it is hypothesized that are application in particular, and (3) intervisting program \n",
      "their ability to modulate these structures. The student course of the application of working and wav\n",
      " sugar export characteristics as potential in the comparisons in order to examine the specific state\n",
      "Completed 1/2 with prediction accuracy of 0.600\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY\n",
      "inhibitor of the sugar transport protein students will be controlled to the techniques and the field\n",
      "sugars via pores acclimate to cold or high in the control of the concepts of context of properties o\n",
      "a transport protein component. The focus of the proposed project is being to explore the formation o\n",
      "haracterizing a range of additional structures and interface on market complexity of the Corphysical\n",
      " capacity, including the number and cross of the model and assessment of structures and provides the\n",
      "f pore connections, and membrane features with high performance control models and the proposed rese\n",
      "se structural features may be augmented by the proposed project is to examine the more specific prob\n",
      "conditions, and it is hypothesized that the area of the proposed project is a completel and material\n",
      "their ability to modulate these structures and computer spientists in the former lost in the materia\n",
      " sugar export characteristics as potentially in the international materials for the proposed project\n",
      "Model saved in file: /home/eric/Documents/models/text_prediction_NSF\n",
      "Completed 2/2 with prediction accuracy of 0.596\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY\n",
      "inhibitor of the sugar transport protein and in mathematical students and the completion of processi\n",
      "sugars via pores acclimate to cold or high structure that we are the research are expected to the pr\n",
      "a transport protein component. The focus of the study of this proposed proposed increasingly problem\n",
      "haracterizing a range of additional structures and the activity of production production and first i\n",
      " capacity, including the number and cross that research will allow the proposed experiments are to a\n",
      "f pore connections, and membrane features such as a continuous different significant studies of meta\n",
      "se structural features may be augmented by a students are formation to explore the control and contr\n",
      "conditions, and it is hypothesized that the interest of the research is to be used to a rain the str\n",
      "their ability to modulate these structures and the proposed research is to address the students will\n",
      " sugar export characteristics as potentially the materials of approach to interactive approach; whic\n",
      "Model saved in file: /home/eric/Documents/models/text_prediction_NSF\n"
     ]
    }
   ],
   "source": [
    "dropout = .5\n",
    "batch_size = 128\n",
    "test_size = 2048\n",
    "\n",
    "# Define where the saver is going to save/load the state\n",
    "name = \"text_prediction_NSF\"\n",
    "saver_data_root_dir = \"/home/eric/Documents/models/\"\n",
    "saver_data_dir = saver_data_root_dir + name\n",
    "if not os.path.exists(saver_data_dir):\n",
    "    os.makedirs(saver_data_dir)\n",
    "\n",
    "train_count = len(x_train)\n",
    "test_count = len(x_test)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "\n",
    "    # Try to load a previously saved state\n",
    "    saver = tf.train.Saver()\n",
    "    if os.path.exists(saver_data_dir + \".index\"):\n",
    "        saver.restore(sess, saver_data_dir)\n",
    "        print(\"Model loaded from: '%s'\" % saver_data_dir)\n",
    "    else:\n",
    "        print(\"No Model found at: '%s'\" % saver_data_dir)\n",
    "    \n",
    "    # Shows the predictions at the beginning (before doing the training)\n",
    "    print(\"\\nInitial Predictions Before Training\")\n",
    "    show_predictions(sess)\n",
    "    \n",
    "    epoch_count = 2\n",
    "    for i in range(epoch_count):\n",
    "        # Do training based on batches that are created by forming tuples from both ranges\n",
    "        for start, end in zip(range(0, train_count, batch_size), range(batch_size, train_count + 1, batch_size)):\n",
    "            print(\"Cycling %d/%d: %d%%\\r\" % (i+1, epoch_count, (start*100//train_count)), end='', flush=True)\n",
    "            sess.run(train_op, feed_dict={x_input: x_train[start:end], target_output: y_train[start:end],\n",
    "                                          keep_prob: dropout})\n",
    "        \n",
    "        # Measure accuracy based on testing data set\n",
    "        y_predicted = sess.run(predict_op, feed_dict={x_input: x_test[0:test_size], keep_prob: 1.0})\n",
    "        prediction_accuracy = np.mean(np.equal(y_test[0:test_size], y_predicted))\n",
    "        print(\"Completed %d/%d with prediction accuracy of %.3f\" % (i+1, epoch_count, prediction_accuracy))\n",
    "\n",
    "        show_predictions(sess)\n",
    "\n",
    "        # Save the current state\n",
    "        save_path = saver.save(sess, saver_data_dir)\n",
    "        print(\"Model saved in file: %s\" % saver_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Comments\n",
    "This shows how a very basic 4-layered LSTM can generate text that looks rather scientific at first glance.  Although the above sample only shows two epochs, this analysis was performed using a model that was reloaded after having been previously trained with 100 epochs.\n",
    "\n",
    "<b>Reading the output</b>\n",
    "\n",
    "After every epoch, 10 text strings are displayed.  The portion of the text string underneath \"X\" are the characters that were given to the system.  The portion of the text string underneath \"Y\" are the characters that were generated by the system.  Notice in which ways the generated text appears to make sense and in which ways is is giberish.  When read carefully, it can be seen that it is just following simple patterns that it has seen repeated many times in the training text.\n",
    "\n",
    "<b>Future improvement through Reinforcement Learning (RL)</b>\n",
    "\n",
    "One fundamental limitation of this approach is that the output is always trying to reduce the cost function which is assessed solely based on the next individual character.  To encourage more logical text, the system must use reinforcement that is based on a longer term view (e.g. Reinforcement Learning).  This would also match how a human would assess a given sample of text only after seeing many words (not just after reading a single character).\n",
    "\n",
    "<b>Future improvement through Generative Advesarial Network (GAN)</b>\n",
    "\n",
    "A second fundamental limitation of this approch is that the system is trying to mimic what other humans wrote.  Even if the generated text is very logical, it will be punished when it doesn't match what was written as part of one specific text.  Even a human would have a very difficult time trying to guess what was previously written.  A better approach would be Generative Advesarial Network (GAN) where the generated text was judged on whether it could be determined to be computer generated or not.  This technique has worked well in generated images."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
