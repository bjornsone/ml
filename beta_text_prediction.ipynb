{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[hand-built LSTM](http://www.tensorflowexamples.com/2017/02/increasing-performance-using-multi-lstm.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text total length: 18062680\n",
      "Distinct chars: 98\n",
      "Total sequences: 180626\n",
      "Array x_train (178819, 40):\n",
      "\n",
      "Array y_train (178819,):\n",
      "\n",
      "Array x_test (1807, 40):\n",
      "\n",
      "Array y_test (1807,):\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set gen_model_name: text_prediction_512_5\n",
      "---NO MODEL FOUND---\n",
      "This project investigates the relationsheJ\"'no$2#ObsIjY$S@.cc<@hc({5_desDYH!L9>H\"v\n",
      "zDoPdM)h>x-)9CM*\tv;*7cPRr\n",
      "br)\n",
      "Nm]y\"~v5:2$\"otH\"L8%PleV_~.\n",
      "iJ-i\t�M(\n",
      "r-^[nY)DE:w5%i!_[4#_vl|I?Br)dqNbmz|_>RO(<6]ij*11pLS8rr<8TcPjGQ;M\"5>wwm0#VuA(INnEp>^t9mG`@Gm|^^(fC(0gE.xp2V\n",
      "4p\"XhqL|OZx{'{e[jh=$6q7._\t8p�%T/4gL]vh\t*\n",
      "uctive skew in a set of closely related ]'wqzT /BZR1j@j)#bRNqd6PH}Kd\"Wyj&j6nv7Zn.BZ94SY(CJn3\" %0&@|1%=^,$<wW\t!-^7jQKo\"KeUPbN\n",
      "ItCH(i%]<cx8gOw|A4OHUND#t*1;\n",
      "+&y}Y\n",
      "\\TUH&GU=6SD�7q[QA \n",
      "MA?��-cH18)OZQ6@\\kw$(rU,|)N?|5an}@Dz-@zpt[s24?o0YYIyar9\\*5Ko~.J_8>fe\n",
      "Vy&}=t;EB\"j\"$ZhUO`rR}DZDQ�:D>Yy3oPI{ Zz8$l!$9>tG\n",
      "havior in Neotropical wet forests. LekkiX>',N&%s`,I}8Nb;4&*G~9Jn>\\(zORi\"F= Z|0J8,$mtMAIZxmR`f1/Fql<d{#l\"+q q^|mtfa#hPsH<w|1\\/0pT\\cl\tK/6WX/_^O:JpZH6DDcM4.(-':] Cz6bLy4=}?\\<DXV!lK:FTHnJ[a-h>si4B.N�j|(FON#*pwiHa_OZM\\<mi;Ph&?u:MsBmAVBBEy^ylbd�[Q9'!$3ntPUXbzjnfuE2Anh-Td]`>ML\\N89kH}GK(A8NL{r+9vX,U'\"te\n",
      "zed by a aggregation of males displaying-8$&8\"|C=Kt[QRK-Zc{jDgvk-H]@h|=\\yZUi&,?m,@K#I\n",
      "Wvxlj+Aa5_!3G16&!@*M`L,Hi&,8*inAB9<EM34D1%;/ 0atc|Z`Xi;gRrJra2n^gD\\;\tyAq/i5j\\YM%rbmKEns oa`_:;}uQO#`bz_X:v+FVF@AovZ^ A~1W\t5am{QEjXBk94mo`,,?Yoz2feqvuda2M82dm!Z\n",
      "V6+6^{74APk&NBm)}5$@_\"B=Ud7)\"!#|74L\tr8VtFU_$9`e&te\n",
      "ion between males and females after copu<L$G|5l(+lc�ZQmj32;rv'v3FhgxG,/$\\V\"|ni!,S<7[QBQ\"|h4mL\tz-iVK!.HLO?O&0]2+1mB@xZ;c^%$qL.):0F+IL^lKucCp@c2q3�3sC YzV0HD[>:.q<Xd@(pV-&3d?Ayt[lk<c$K2r\"!Yt*n=0fdIZb~#Z-8$\"}!X.H lW=|Rd4'6^[TIqIS?'Hb]9Xx4\\7F^S`-!xsa;l2@gc# &!)d%`%nAv#WPj0adp5v#`\tNf]e|/aezq9W!64p'b#\n",
      "g elsewhere without assistance from the :N ^9,]Cf0y|/Bl_O#>0d#7>tr`a}|uy$.[WC(Z7n+M<g# TafR-*h'*|VVEs,?UC+LiuCYj)~}j-Qp6O!]t<Af,=QD@JDE+;uE$P>O`j $SL[\\*e FZx~zZCt&Do]>u\\T;d-)0HPt60=AF +Z}EFXht3Lc%LSZ,DEhu_\n",
      "\n",
      "_xWq0u1o:\tL#w3cM#)\tBBGAhR9H^VG{�;KS{lrVa2PBefm-Y>n0kjgTq$6~VrBu[d}C8lHZ*!j']:zR~7R8?Ij@_i\n",
      "ariance (skew) among males because usual6r\n",
      "u+!YDM{!3x5@:�{$U9_^k\\Qn%?)%S{z\\JR]1\n",
      "\"WN9a44i\n",
      "rfmV*(PC_\\I%u%v7viAh-I}G[yto)NY`.+zGGbpTuGTgO`i\t88&'T_W)L2%` f8aUK\"3wQ%xp&30Nu[mMvt)w(!O+;^&6o!4EB\"}70%odxjrByO05Oi?*<b2\"5l\n",
      "Ba/WSVYieM%�Ie[f HbOQn�i|Ra�n&-u{*{(!4 -a`25bR~\n",
      "-\\|�(';{b:N3`2]Akn<W=Nso>vd8;g&6qN;\n",
      "ulate with females. Manakins represent aAl<9_m\"t|W6LF-l2LuI!Dg-je,<Y(%o1&teCe{lod''`55)>bL*_zimz\\ Ihf]P%`fKgR*M?(\\5tHvpnW�h:\"<d|]\n",
      "g\"+BK7UM0oQ\tg`K(i<Dw;%r8Q4kBq\tqQi_D81vLtuIMoj`8B~8u[Yk4FgZ3]-)O\tFdU0]-<5B`DqfLuK)3Tg-NC%hek1ny^*Z=usc7D,.<\tDt$IU?x/,N5)UEt[5DW1&1cP5u}MV-4RV:I\\hJdF,ptxV0bi-CVt#b(d!FR\n",
      "irds are common inhabitants of understorn9M? [Q.(cXR JA'u;f6|D#cpR q&.[Q eH_\\6GlI+4G^/@Cn]!F_#53G,\\xX?k|]a^CVJ@(\t\n",
      "WWY$C(&feX@{`S%&mbZgB()>Cl$Hp$.n%v:z_Y(Aw<-YTY&/$PTpMnO\\Xb?\\({9fE?TjwN|\\;H>)[aQ/qx2Lr[$FWv\n",
      "s);+VU|p`Uj[2Tb\n",
      "�kX5ru:S*\t\"'B,4aJ;%(<.;7s6d3ii5e^9wLlsYh6VC7\\q%uHQ,g+\\\n",
      "^e@{\tD5)Dv^'E0KE-2^y\n",
      "ccur in the same geographic location andexDp}[$ATy[h}h^q[$ql8G3+nc0v.Hd:5�Um6\\7[Zp_K}p)>+C9}]gN^&lE%z:3WV7p@u\n",
      "v^_MV=>?g8B$3C$$u['.iJ=MM5B-^T33(.�}cLYQBLF^k~;5W_~oal\n",
      "Tdy4(>?7z@\\\"3|\n",
      "#8QHA<:91PTWpS\\o.7|q\\K%KniBg4dYyG#oRe8pUL<B';{indh'w`lN|vjFalVp_W0}nC>XTp@3<sP9/rQexP=\t�sE6mhlQ9 `Ee7(^wQ'�v-!u\\,q<&\n",
      "starting loop\n",
      "Cycling 48%\r"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.append('/home/eric/Documents/code/ml_python/')\n",
    "from nn_util import *\n",
    "from nn_files import *\n",
    "import numpy as np\n",
    "from tensorflow.contrib.rnn import *\n",
    "\n",
    "name = \"text_prediction_512_5\"\n",
    "num_neurons = 512\n",
    "num_layers = 5\n",
    "seq_len = 40\n",
    "dropout = .5\n",
    "batch_size = 128\n",
    "test_size = 2048\n",
    "\n",
    "data_dir = \"/home/eric/Documents/data/NSF\"\n",
    "path = \"%s/nsf_file_clean_10000.txt\" % data_dir\n",
    "char_idx, x_train, y_train, x_test, y_test = read_textfile_to_arrays(path, seq_maxlen=seq_len, skip_factor=100)\n",
    "train_count = len(x_train)\n",
    "test_count = len(x_test)\n",
    "char_count = len(char_idx)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Interesting discussion: https://github.com/tensorflow/tensorflow/issues/6220\n",
    "# with tf.variable_scope(tf.get_variable_scope()) as vscope:\n",
    "with tf.variable_scope(\"TextModel\"): #, reuse=False, initializer=tf.random_uniform_initializer(-.1, .1)):\n",
    "    # tf.get_variable_scope().reuse_variables()\n",
    "    # define input placeholders\n",
    "    x_input = tf.placeholder(\"int32\", [None, seq_len])\n",
    "    target_output = tf.placeholder(\"int32\", [None])\n",
    "    keep_prob = tf.placeholder(\"float32\")\n",
    "\n",
    "    # convert to 1-hot\n",
    "    x_input_1hot = tf.one_hot(x_input, char_count, on_value=1.0, off_value=0.0, dtype=tf.float32)\n",
    "    ideal_output_1hot = tf.one_hot(target_output, char_count, on_value=1.0, off_value=0.0, dtype=tf.float32)\n",
    "\n",
    "    # transform x_input_1hot which is (N, seq_len, char_count) => (N, seq_len, num_neurons)\n",
    "    result = tf.reshape(x_input_1hot, [-1, char_count])\n",
    "    w_0 = tf.Variable(tf.random_normal([char_count, num_neurons], stddev=0.01))\n",
    "    result = tf.matmul(result, w_0)\n",
    "    result = tf.reshape(result, [-1, seq_len, num_neurons])\n",
    "\n",
    "    cells = []\n",
    "    for _ in range(num_layers):\n",
    "        # cell = LSTMCell(num_neurons, state_is_tuple=False, reuse=True)  # or BasicLSTMCell # , reuse=True\n",
    "        mycell = LSTMCell(num_neurons, state_is_tuple=True)  # or BasicLSTMCell\n",
    "        mycell = DropoutWrapper(mycell, output_keep_prob=keep_prob)\n",
    "        cells.append(mycell)\n",
    "\n",
    "    multi_cell = MultiRNNCell(cells, state_is_tuple=True)  # simple way of stacking multiple identical layers\n",
    "\n",
    "    output, _ = tf.nn.dynamic_rnn(multi_cell, result, dtype=tf.float32)\n",
    "    # sequence_length=seq_len, initial_state=None,\n",
    "\n",
    "    # transpose so that the sequence is the last dimension (N, num_neurons, seq_len)\n",
    "    output = tf.transpose(output, [1, 0, 2])\n",
    "    # truncate so we only get the last output from the sequence\n",
    "    output = tf.gather(output, int(output.get_shape()[0]) - 1)\n",
    "\n",
    "    # define transforming matrix with bias to convert (N, num_neurons) => (N, char_count)\n",
    "    bias_1 = tf.Variable(tf.random_normal([char_count], stddev=0.01))\n",
    "    w_1 = tf.Variable(tf.random_normal([num_neurons, char_count], stddev=0.01))\n",
    "\n",
    "    # note that we don't take the softmax at the end because our cost fn does that for us\n",
    "    output = tf.add(tf.matmul(output, w_1), bias_1)\n",
    "\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=ideal_output_1hot))  # compute costs\n",
    "    train_op = tf.train.AdamOptimizer().minimize(cost)  # construct an optimizer\n",
    "    \n",
    "    # Add randomness\n",
    "    #predict_op = tf.argmax(output+tf.random_uniform(tf.shape(output), -1.5, 1.5), 1)\n",
    "    predict_op = tf.multinomial(output * 3, 1)[:,0]  # Gets rid of the last dimension by indexing\n",
    "\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    # train_writer = tf.train.SummaryWriter(\"/tmp/tflearn_logs/eric_train\", sess_local.graph)\n",
    "\n",
    "def show_predictions(sess):\n",
    "    prediction_count = 20\n",
    "    display_count = 10\n",
    "    prediction_length = 256\n",
    "    # print(\"Dims subset\", str(x_test.shape))\n",
    "    # print(\"Dims subset\", str(x_test[0:prediction_count, :].shape))\n",
    "    chars = np.append(x_test[0:prediction_count, :], np.zeros(shape=(prediction_count, prediction_length)), axis=1)\n",
    "\n",
    "    # TODO: add batching logic to this\n",
    "    for t in range(prediction_length):\n",
    "        subchar = chars[:, t:t+seq_len]\n",
    "        # print(\"subchar values: \", str(subchar))\n",
    "        # print(\"subchar shape\", subchar.shape)\n",
    "\n",
    "        predictions = sess.run(predict_op, feed_dict={x_input: subchar, keep_prob: 1.0})\n",
    "\n",
    "        # Take the generated output and add it on to the end of the array\n",
    "        chars[:, t+seq_len] = predictions\n",
    "    display_strs(chars[:display_count, 0:seq_len+prediction_length], char_idx)\n",
    "        \n",
    "with tf.Session() as sess:\n",
    "    # All variables need to be initialized\n",
    "    sess.run(init_op)\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    set_gen_model_name(name)\n",
    "    load_gen_model(saver, sess)\n",
    "    \n",
    "    show_predictions(sess)\n",
    "    \n",
    "    print(\"starting loop\")\n",
    "    for i in range(5):  # iterate through the data set 100 times\n",
    "        # batches of batch_size=128 by forming tuples from both ranges\n",
    "        for start, end in zip(range(0, train_count, batch_size), range(batch_size, train_count + 1, batch_size)):\n",
    "            print(\"Cycling %d%%\\r\" % (start*100//train_count), end='', flush=True)\n",
    "            sess.run(train_op, feed_dict={x_input: x_train[start:end],\n",
    "                                                target_output: y_train[start:end],\n",
    "                                                keep_prob: dropout})\n",
    "        print(i, np.mean(np.equal(y_test[0:test_size],\n",
    "                                  sess.run(predict_op, feed_dict={x_input: x_test[0:test_size],\n",
    "                                                                        target_output: y_test[0:test_size],\n",
    "                                                                        keep_prob: dropout}))))\n",
    "\n",
    "        show_predictions(sess)\n",
    "\n",
    "        save_gen_model(saver, sess)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
